---
title: "Iteratively Improving Scientific Integrity"
date: 2025-10-15
---
# 1. Introduction
[link](https://en.wikipedia.org/wiki/Richard_Feynman)Richard Feynman</a> received the Nobel Prize in Physics in 1965 for his contributions to the development of quantum electrodynamics. In 1974, he gave a Caltech [link](ttps://faculty.sites.iastate.edu/tesfatsi/archive/tesfatsi/CargoCultScience.RichardFeynman1974.pdf)commencement address</a> in which he told a story of a society that wanted the benefits of scientific progress, and did what it imagined were the necessary steps for that progress, but progress never happened because they lacked “scientific integrity.” Feynman explains,
  
> It's a kind of scientific integrity,
a principle of scientific thought that corresponds to a kind of utter honesty--a kind of
leaning over backwards. For example, if you're doing an experiment, you should report
everything that you think might make it invalid--not only what you think is right about
it: other causes that could possibly explain your results; and things you thought of that
you've eliminated by some other experiment, and how they worked--to make sure the
other fellow can tell they have been eliminated.
>
> Details that could throw doubt on your interpretation must be given, if you know them.
You must do the best you can--if you know anything at all wrong, or possibly wrong--
to explain it. If you make a theory, for example, and advertise it, or put it out, then you
must also put down all the facts that disagree with it, as well as those that agree with it. [...]
>
> In summary, the idea is to give all of the information to help others to judge the value
of your contribution; not just the information that leads to judgement in one particular
direction or another. [...]
>
> But this long history of learning how to not fool ourselves--of having utter scientific
integrity--is, I'm sorry to say, something that we haven't specifically included in any
particular course that I know of. We just hope you've caught on by osmosis.

Going beyond the hope of learning scientific integrity through osmosis, this essay proposes that Google's new <a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist">co-scientist</a> AI agent is a practical approach to the widespread development of scientific integrity, as Feynan had hoped.  

# 2. Previous work

 
# 3. Architecture


# 4. Test case


# 5. Other candidate test cases


# 6. Conclusion

<a href="https://en.wikipedia.org/wiki/Paul_Buchheit">Paul Buchheit</a> was an early engineer (i.e., employee #23) at <a href="https://en.wikipedia.org/wiki/Google">Google</a> before leaving in 2006 to co-found FriendFeed with <a href="https://en.wikipedia.org/wiki/Bret_Taylor">Bret Taylor</a>, now the chairman of <a href="https://en.wikipedia.org/wiki/OpenAI">OpenAI</a>. In 2009, <a href="https://en.wikipedia.org/wiki/Facebook">Facebook</a> acquired FriendFeed but Buchheit left Facebook in 2010 to become a partner at <a href="https://en.wikipedia.org/wiki/Y_Combinator">Y Combinator</a>. Buchheit is a thought-leader concerning technology's impact on society. In this <a href="https://youtu.be/LSUviaN1eso?t=874">YouTube interview</a> at Y Combinator, Buchheit talks about two possible outcomes of AI:
<blockquote>
  <p>When we think about what is
the long-term trajectory of AI, it's the most powerful technology we've ever
invented. So the question is like where does that power go? And I think there's essentially two directions: you
either go towards centralization where all the power gets centralized in the government or in a small 
number of big tech companies or something like that and my feeling is that that's catastrophic for the human
species because you essentially minimize the agency and power of the individual, and I think the opposite
direction is towards freedom and, as much as possible, we should give this
power and these capabilities to every individual to be the best version of themselves</p>
</blockquote>
<p>There is plenty of evidence in recent news that the outcome for AI will be centralized power:</p>
